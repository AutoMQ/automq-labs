import marimo

__generated_with = "0.18.0"
app = marimo.App(width="medium", app_title="GitHub Real-Time Analytics")


@app.cell(hide_code=True)
def _():
    # Import required libraries
    import marimo as mo
    import os

    # ç®€åŒ–åŒ…é…ç½®ï¼Œä½¿ç”¨ tabulario/spark-iceberg é•œåƒé¢„è£…çš„åŒ…
    # ä¸éœ€è¦é¢å¤–ä¸‹è½½åŒ…ï¼Œç›´æŽ¥ä½¿ç”¨é•œåƒä¸­å·²æœ‰çš„é…ç½®
    print("âœ“ Using pre-configured Spark packages from tabulario/spark-iceberg image")

    # Now import pyspark after setting environment variable
    try:
        print("Importing PySpark...")
        from pyspark.sql import SparkSession
        print("âœ“ PySpark imported successfully")

        # Initialize SparkSession using pre-configured environment
        print("Initializing SparkSession using pre-configured environment...")
        spark = SparkSession.builder \
            .appName("GitHub Events Analytics") \
            .getOrCreate()
        print("âœ“ SparkSession created successfully")
        print(f"âœ“ Spark version: {spark.version}")
        # Uncomment the line below to test query on startup
    except Exception as e:
        error_msg = f"""âŒ Error initializing SparkSession:{str(e)}"""
        print(error_msg)
        raise RuntimeError(error_msg) from e

    return mo, spark, SparkSession


@app.cell(hide_code=True)
def _(mo):
    import time
    import threading
    import schedule
    
    # çŠ¶æ€ç®¡ç†
    get_events, set_events = mo.state(value=[])
    get_spark_session, set_spark_session = mo.state(value=None)
    get_restart_count, set_restart_count = mo.state(value=0)
    get_scheduler_started, set_scheduler_started = mo.state(value=False)
    get_last_update, set_last_update = mo.state(value="Never")
    
    return (get_events, set_events, get_spark_session, set_spark_session, 
            get_restart_count, set_restart_count, get_scheduler_started, 
            set_scheduler_started, get_last_update, set_last_update, time, threading, schedule)


@app.cell
def _(mo):
    mo.md(r"""
    # ðŸš€ GitHub Events Real-Time Analytics
    This demo showcases **[AutoMQ](https://github.com/AutoMQ/automq) Table Topic** - automatically converting Kafka topics into Apache Iceberg tables for real-time analytics.

    **Data Source**: [GH Archive](https://www.gharchive.org/) - Public GitHub timeline events  
    **Technology**: AutoMQ Table Topic (Zero ETL, Real-time Ingestion)
    ---
    """)
    return


@app.cell(hide_code=True)
def _(get_spark_session, set_spark_session, get_restart_count, set_restart_count, 
      get_scheduler_started, set_scheduler_started, get_events, set_events, get_last_update, 
      set_last_update, threading, schedule, time, SparkSession, spark):
    
    def refresh_data():
        print('refresh event data')
        """åˆ·æ–°æ•°æ®çš„å‡½æ•°"""
        try:
            # ä½¿ç”¨å½“å‰å¯ç”¨çš„ Spark ä¼šè¯
            current_spark = get_spark_session() if get_spark_session() is not None else spark
            
            # Query the latest data
            df = current_spark.sql("SELECT * FROM default.github_events_iceberg ORDER BY created_at DESC LIMIT 20")
            pandas_df = df.toPandas()
            
            # Update state
            set_events(pandas_df)
            
            # Update last refresh time
            current_time = time.strftime("%H:%M:%S")
            set_last_update(current_time)
            
            print(f"ðŸ”„ [Auto-refresh] Data updated at {current_time} - Found {len(pandas_df)} records")
            
        except Exception as e:
            print(f"âŒ [Auto-refresh] Error refreshing data: {e}")
    
    def restart_spark():
        """é‡å¯ Spark ä¼šè¯çš„å‡½æ•°"""
        print(f"ðŸ”„ [Background] Auto-restarting Spark (restart #{get_restart_count() + 1})...")
        try:
            # åœæ­¢å½“å‰ Spark ä¼šè¯
            current_spark = get_spark_session()
            
            # ç­‰å¾…ä¸€ä¸‹
            time.sleep(3)
            
            # åˆ›å»ºæ–°çš„ Spark ä¼šè¯
            new_spark = SparkSession.builder \
                .appName(f"GitHub Events Analytics - Auto Restart {get_restart_count() + 1}") \
                .getOrCreate()

            if current_spark is not None:
                current_spark.stop()
                print("âœ“ [Background] Previous Spark session stopped")
            
            # æ›´æ–°çŠ¶æ€
            set_restart_count(get_restart_count() + 1)
            set_spark_session(new_spark)
            
            print(f"âœ“ [Background] New Spark session created")
            print(f"âœ“ [Background] Spark version: {new_spark.version}")
            
        except Exception as e:
            print(f"âŒ [Background] Error restarting Spark: {e}")
    
    def run_scheduler():
        """è¿è¡Œè°ƒåº¦å™¨çš„åŽå°çº¿ç¨‹å‡½æ•°"""
        while True:
            schedule.run_pending()
            time.sleep(1)
    
    # å¯åŠ¨åŽå°è°ƒåº¦å™¨ï¼ˆåªå¯åŠ¨ä¸€æ¬¡ï¼‰
    if not get_scheduler_started():
        print("ðŸš€ Starting background schedulers...")
        
        # è®¾ç½®æ¯10ç§’åˆ·æ–°æ•°æ®
        schedule.every(10).seconds.do(refresh_data)
        
        # è®¾ç½®æ¯10åˆ†é’Ÿé‡å¯Spark
        schedule.every(30).minutes.do(restart_spark)
        
        # å¯åŠ¨åŽå°çº¿ç¨‹
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        
        # æ ‡è®°è°ƒåº¦å™¨å·²å¯åŠ¨
        set_scheduler_started(True)
        
        print("âœ“ Background schedulers started:")
        print("  â€¢ Data refresh: every 10 seconds")
        print("  â€¢ Spark restart: every 10 minutes")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ•°æ®åˆ·æ–°
        refresh_data()
    else:
        print("âœ“ Background schedulers already running")


@app.cell
def _(mo, get_last_update):
    mo.vstack([
        mo.md("## ðŸ“Š Live GitHub Events Data"),
        mo.md(f"*Last updated: {get_last_update()} â€¢ Auto-refresh every 10 seconds*")
    ])


@app.cell(hide_code=True)
def _(get_events, set_events, spark, get_spark_session, set_last_update, time):
    # åˆå§‹æ•°æ®åŠ è½½ï¼ˆåªåœ¨é¦–æ¬¡å¯åŠ¨æ—¶æ‰§è¡Œï¼‰
    if len(get_events()) == 0:
        print("ðŸ” Initial data loading...")
        try:
            # ä½¿ç”¨å½“å‰å¯ç”¨çš„ Spark ä¼šè¯
            current_spark = get_spark_session() if get_spark_session() is not None else spark
            
            # Query the data
            df = current_spark.sql("SELECT * FROM default.github_events_iceberg ORDER BY created_at DESC LIMIT 20")
            pandas_df = df.toPandas()
            
            # Store the data in state
            set_events(pandas_df)
            set_last_update(time.strftime("%H:%M:%S"))
            
            print(f"âœ“ Initial data loaded - Found {len(pandas_df)} records")
            
        except Exception as e:
            print(f"âŒ Error loading initial data: {e}")
            print("ðŸ’¡ Background scheduler will handle data refresh automatically")
            set_events([])
    else:
        print("âœ“ Data already loaded, background refresh is active")


@app.cell
def _(get_events, mo):
    events_data = get_events()
    # Select only specific columns to display (modify this list as needed)
    display_columns = ['id', 'type', 'actor_login', 'repo_name', 'created_at']


    # Filter the DataFrame to show only selected columns
    if len(events_data) > 0:
        # Check which columns actually exist in the data
        available_columns = [col for col in display_columns if col in events_data.columns]
        filtered_data = events_data[available_columns]
    
        print(f"âœ“ Displaying {len(available_columns)} columns: {', '.join(available_columns)}")
    else:
        filtered_data = events_data

    # Create interactive table with filtered data
    table = mo.ui.table(
        filtered_data,
        selection=None,
        show_column_summaries=False
    )

    # Return the vstack as the final expression to display
    mo.vstack([
        mo.md("### Recent GitHub Events"),
        table
    ])
    return


if __name__ == "__main__":
    app.run()
